## 1. 逻辑回归与线性回归的联系与区别
方法 | 自变量（特征） | 因变量（结果） | 关系 | 一般表达式 |代价函数 
:-: |:-: |:-: |:-:|:-:|:-:|
linear | 连续或离散 | 连续实数 | 线性 | f(x) = wx<sub>i</sub> + b | 均方误差
Logistic | 连续或离散 | (0,1)之间连续值 | 非线性 |![](http://latex.codecogs.com/gif.latex?g(z)=\\frac{1}{1+e^{-z}}) | 极大似然

  ![](https://github.com/lxrobot/lxrobot-s-code/blob/master/linear.jpg?raw=true)  
> 总之，logistic回归与线性回归实际上有很多相同之处，最大的区别就在于他们的**因变量不同**，其他的基本都差不多，正是因为如此，这两种回归可以归于同一个家族，即广义线性模型（generalized linear model）。这一家族中的模型形式基本上都差不多，不同的就是因变量不同，**如果是连续的，就是多重线性回归**，**如果是二项分布，就是logistic回归**。logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最为常用的就是二分类的logistic回归。  
## 2. 逻辑回归原理
简单来说，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达到将数据二分类的目的。  
具体来说，在线性回归中，因变量是连续变量，那么线性回归能够根据因变量和自变量之间存在的线性关系来构造回归方程；但是，一旦因变量是分类变量，那么因变量与自变量就不会存在上面这种线性关系了，这时候就要通过某种变换来解决这个问题，这个变换就是对数变换；**逻辑回归是在线性回归的基础上套用了一个逻辑函数，y值控制在0到1之间。**  
逻辑回归可以看做是两步，第一步和线性回归模型的形式相同，即一个关于输入x 的线性函数：  
**Z = w<sup>T</sup>x + b**  
第二步通过一个逻辑函数，即sigmoid函数，将线性函数转换为非线性函数。  
**![](http://latex.codecogs.com/gif.latex?g(z)=\\frac{1}{1+e^{-z}})**  
**注意：** 逻辑回归用于分类（一般为二分类），而不用于回归！
## 3. 逻辑回归损失函数推倒及优化
逻辑回归的损失函数是其极大似然函数。
#### 损失函数的推倒：
已知估计函数为：

则似然概率分布为（即输出值为判断为1的概率，但在输出标签值时实际只与0.5作比较）：

可以写成概率一般式：
## 4. 正则化与模型评估指标
- **正则化**  
正则化就是在损失函数后加上一个正则化项（惩罚项），其实就是常说的结构风险最小化策略，即经验风险（损失函数）加上正则化。一般模型越复杂，正则化值越大。 
正则化是用来对模型中某些参数进行约束，目的是防止过拟合。
正则化的一般形式：

- **评估指标**  
**分类模型：** 准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F<sub>1</sub>-sore(精确率和召回率的调和平均) 、AUC（ROC曲线下面积） 
**回归模型：** 平方根误差（RMSE）、误差分位数（Quantiles of Errors）、Almost Crrect” Predictions 
## 5. 逻辑回归的优缺点  
**优点：**  
- 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。  
- 模型效果不错。在工程上是可以接受的（作为 baseline），如果特征工程做的好，效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。  
- 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化 SGD 发展比较成熟。  
- 方便调整输出结果，通过调整阈值的方式。     

**缺点：**  
- 准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。
- 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1。我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。  
- 无法自动的进行特征筛选。 
- 只能处理二分类问题。 
## 6. 样本不均衡问题解决办法  
- **样本的过采样和欠采样**  
**过采样：** 将稀有类别的样本进行复制，通过增加此稀有类样本的数量来平衡数据集.该方法适用于数据量较小的情况.    
**欠采样：** 从丰富类别的样本中随机选取和稀有类别相同数目的样本，通过减少丰富类的样本量啦平衡数据集.该方法适用于数据量较大的情况.    
**SMOTE构造采样:** 基于距离度量的方式计算两个或多个稀有类样本之间的相似性.然后选择其中的一个样本作为基础样本，再在邻居样本中随机选取一定数量的样本对那个基础样本的一个属性进行噪声.每次处理一个属性，通过这样的方式产生新生数据.      
- **对原数据的权值进行改变**    
通过改变多数类样本和少数类样本数据在训练时的权重来解决样本不均衡的问题，是指在训练分类器时，为少数类样本赋予更大的权值，为多数类样本赋予较小的权值.  
- **通过组合集成方法解决**  
将多数类数据随机分成少数类数据的量N份，每一份与全部的少数类数据一起训练成为一个分类器，这样反复训练会生成很多的分类器。最后再用组合的方式(bagging或者boosting)对分类器进行组合，得到更好的预测效果。简单来说若是分类问题可采用投票法，预测问题可以采用平均值。这个解决方式需要很强的计算能力以及时间，但效果较好，相当于结合了组合分类器的优势.    
- **通过特征选择**    
在样本数据较为不均衡，某一类别数据较少的情况下，通常会出现特征分布很不均衡的情况。例如文本分类中，有大量的特征可以选择。因此我们可以选择具有显著区分能力的特征进行训练，也能在一定程度上提高模型的泛化效果.  
## 7. sklearn 参数  

### 附：相关知识复习
1. **log && lg && ln**
  - 如果a(a>0，且a≠1)的b次幂等于N，即ab=N，那么数b叫做以a为底N的对数，记作：**log<sub>a</sub>N=b** ,其中a叫做对数的底数，N叫做真数.  
  - 以10为底的对数叫常用对数，记作 **log<sub>10</sub>N = lgN** .    
  - 以无理数e(e=2.718 28…)为底的对数叫做自然对数，记作 **log<sub>e</sub>N = lnN**.  
2. **二阶导数**

 
 ### 参考资料
 [逻辑回归与线性回归的区别与联系](https://blog.csdn.net/lx_ros/article/details/81263209)  
 [模型评价指标1](https://blog.csdn.net/liulina603/article/details/78901160)、 [模型评价指标2](https://blog.csdn.net/zuolixiangfisher/article/details/81328297)、 [模型评价指标3](https://blog.csdn.net/weixin_37294079/article/details/79004835)   
 [正则化1](https://blog.csdn.net/m0_37952909/article/details/79686573)
 [正则化2](https://blog.csdn.net/haima1998/article/details/79425831)
 [正则化3](https://blog.csdn.net/u012978177/article/details/53084978)  
 [正负样本不均衡解决办法](https://blog.csdn.net/jemila/article/details/77992967)
