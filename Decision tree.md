## 决策树模型   
- **定义：分类决策树模型是一种**描述对实例进行分类的**树形结构**(可以是二叉树或非二叉树).  其中内部节点（非叶子节点）表示一个**特征或属性**，叶子节点表示一个**类**.【或者说，内部节点是每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别】  
**！决策过程！** 使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果.  
决策树可以看成一个**if-then**规则集合，其中**内部结点的特征**对应着**规则的条件**，而**叶节点的类**对应着**规则的结论**.  

![决策树](https://pic2.zhimg.com/80/v2-39d109b46ea4f34d5efbf67edc11d57d_hd.png)
- **如何构建决策树？**  
当给定练数据，由上图可以看出每一次子结点的产生，是由于在当前层数选择了不同的特征来作为分裂因素造成的。每一层选择了指定的特征之后，我们就可以继续由该特征的不同属性值进行划分，依次一直到叶子结点。   
**！！特征是以什么标准来选择的呢？**  
所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。而判断“纯”的方法不同引出了我们的**ID3算法，C4.5算法以及CART算法**，这些下面会详细介绍！
- **如果属性用完了怎么办？**  
在决策树构造过程中可能会出现这种情况：所有属性都作为分裂属性用光了，但有的子集还不是纯净集，即集合内的元素不属于同一类别。在这种情况下，由于没有更多信息可以使用了，一般对这些子集进行“多数表决”，即使用此子集中出现次数最多的类别作为此节点类别，然后将此节点作为叶子节点.    
## 信息论基础（熵、联合熵、条件熵、信息增益、基尼不纯度）
1. **熵**  
在信息论与概率论中，熵是表示随机变量不确定性的度量，设 X 是一个取有限个值的离散随机变量，其概率分布为  
    P(X=x<sub>i</sub>) = P<sub>i</sub>, i = 1,2,...,n  
则**随机变量 X 的熵**定义为  
![](http://latex.codecogs.com/gif.latex?H(X)=-\\sum_{i=1}^{n}p_{i}logp_{i})  
！！熵只依赖于 X 的分布，而与 X 的取值无关, 熵越大，随机变量的不确定性就越大.  


## 决策树的不同分类算法（ID3、C4.5、CART分类树）的原理及应用场景

## 回归树原理

## 决策树防止过拟合手段
- **关于剪枝**

 在实际构造决策树时，通常要进行剪枝，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：

   先剪枝——在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。

   后剪枝——先构造完成完整的决策树，再通过某些条件遍历树进行剪枝

## 模型评估

## sklearn参数详解 python绘制决策树



### 参考文献
1. [深入浅出理解决策树](https://zhuanlan.zhihu.com/p/26703300)
