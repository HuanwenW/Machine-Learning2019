## 决策树模型   
- **定义：分类决策树模型是一种**描述对实例进行分类的**树形结构**(可以是二叉树或非二叉树).  其中内部节点（非叶子节点）表示一个**特征或属性**，叶子节点表示一个**类**.【或者说，内部节点是每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别】  
**！决策过程！** 使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果.  
**！类比！** 决策树可以看成一个**if-then**规则集合，其中**内部结点的特征**对应着**规则的条件**，而**叶节点的类**对应着**规则的结论**.  
![image][https://images2015.cnblogs.com/blog/993405/201610/993405-20161013111749625-528857151.png]
- **如何构建决策树？**  
当给定练数据，由上图可以看出每一次子结点的产生，是由于在当前层数选择了不同的特征来作为分裂因素造成的。每一层选择了指定的特征之后，我们就可以继续由该特征的不同属性值进行划分，依次一直到叶子结点。   
**！！特征是以什么标准来选择的呢？**  
所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。而判断“纯”的方法不同引出了我们的**ID3算法，C4.5算法以及CART算法**，这些下面会详细介绍！
## 信息论基础（熵、联合熵、条件熵、信息增益、基尼不纯度）
1. **熵**  
在信息论与概率论中，熵是表示随机变量不确定性的度量，设 X 是一个取有限个值的离散随机变量，其概率分布为  
    P(X=x<sub>i</sub>) = P<sub>i</sub>, i = 1,2,...,n  
则**随机变量 X 的熵**定义为  
![](http://latex.codecogs.com/gif.latex?H(X)=-\\sum_{i=1}^{n}p_{i}logp_{i})  
！！熵只依赖于 X 的分布，而与 X 的取值无关, 熵越大，随机变量的不确定性就越大.  



## 决策树的不同分类算法（ID3、C4.5、CART分类树）的原理及应用场景

## 回归树原理

## 决策树防止过拟合手段

## 模型评估

## sklearn参数详解 python绘制决策树
