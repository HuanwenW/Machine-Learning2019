## 决策树模型   
- **定义：分类决策树模型是一种**描述对实例进行分类的**树形结构**(可以是二叉树或非二叉树).  其中内部节点（非叶子节点）表示一个**特征或属性**，叶子节点表示一个**类**.【或者说，内部节点是每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别】  
**！决策过程！** 使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果.  
决策树可以看成一个**if-then**规则集合，其中**内部结点的特征**对应着**规则的条件**，而**叶节点的类**对应着**规则的结论**.  

![决策树](https://pic2.zhimg.com/80/v2-39d109b46ea4f34d5efbf67edc11d57d_hd.png)
- **如何构建决策树？**  
当给定练数据，由上图可以看出每一次子结点的产生，是由于在当前层数选择了不同的特征来作为分裂因素造成的。每一层选择了指定的特征之后，我们就可以继续由该特征的不同属性值进行划分，依次一直到叶子结点。   
**！！特征是以什么标准来选择的呢？**  
所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。而判断“纯”的方法不同引出了我们的**ID3算法，C4.5算法以及CART算法**，这些下面会详细介绍！
- **如果属性用完了怎么办？**  
在决策树构造过程中可能会出现这种情况：所有属性都作为分裂属性用光了，但有的子集还不是纯净集，即集合内的元素不属于同一类别。在这种情况下，由于没有更多信息可以使用了，一般对这些子集进行“多数表决”，即使用此子集中出现次数最多的类别作为此节点类别，然后将此节点作为叶子节点.    
## 信息论基础（熵、联合熵、条件熵、信息增益、基尼不纯度）
1. **熵**  
在信息论与概率论中，熵是表示随机变量不确定性的度量，设 X 是一个取有限个值的离散随机变量，其概率分布为  
    P(X=x<sub>i</sub>) = P<sub>i</sub>, i = 1,2,...,n  
则**随机变量 X 的熵**定义为  
![](http://latex.codecogs.com/gif.latex?H(X)=-\\sum_{i=1}^{n}p_{i}logp_{i})  
！！熵只依赖于 X 的分布，而与 X 的取值无关, 熵越大，随机变量的不确定性就越大.  
2. **联合熵**    
联合熵就是度量一个联合分布的随机系统的不确定度，其物理意义是观察一个多个随机变量的随机系统获得的信息量。
3. **条件熵**  
条件熵 H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。条件熵 H(Y|X) 定义为 X 给定条件下 Y 的条件概率分布的熵对  X 的数学期望：  

![条件熵](https://img-blog.csdnimg.cn/20190305205007389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMDc4NTMwNzU4,size_16,color_FFFFFF,t_70)    
条件熵 H(Y|X) 相当于联合熵 H(X,Y) 减去单独的熵 H(X).   
4. **信息增益**   
信息增益=信息熵-条件熵  
信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好.  
5. **基尼不纯度**    
反映从数据集中随机抽取两个样本，其类别标记不一致的概率，基尼不纯度越小，数据集的纯度越高.  
## 决策树的不同分类算法（ID3、C4.5、CART分类树）的原理及应用场景
1. **ID3**  
采用信息增益划分数据，计算获得所有特征中信息增益最大的特征，用于划分数据集.  
ID3 适用场合：二分类  
2. **C4.5**  
ID3算法存在的问题：分裂属性倾向于多值属性，如身份证信息，结果很纯，但是毫无意义.  
C4.5生成树的基准：信息增益比.  
信息增益比=信息增益/信息熵(对应特征).  
克服ID3算法存在的分裂属性倾向问题,其他过程和ID3算法完全一致.  
tips:理想情况下一个分裂子集中待分类项属于同一类别。实际过程中，决策结果如下$(0,0,0,0,1,1)$,以投票原则，该节点输出类别为0.  
C4.5适用场合：非离散数据，数据不完整.  
3. **CART**  
采用基尼指数来选择划分属性。在候选的属性集合中，选择那个使得划分后基尼指数最小的属性作为最优划分属性.

CART全称是分类与回归树，CART不需要对数运算更高效，生成的树必须是二叉树

适用场合：分类问题和回归问题，连续数据

## 回归树原理
![回归树原理](https://blog.csdn.net/weixin_40604987/article/details/79296427)

## 决策树防止过拟合手段
- **关于剪枝**

 在实际构造决策树时，通常要进行剪枝，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：

   先剪枝——在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。

   后剪枝——先构造完成完整的决策树，再通过某些条件遍历树进行剪枝

## 模型评估

## sklearn参数详解 python绘制决策树



### 参考文献
1. [深入浅出理解决策树](https://zhuanlan.zhihu.com/p/26703300)
2. [各种熵](https://www.cnblogs.com/kyrieng/p/8694705.html)
